\chapter{基于GraphCodeBERT模型的智能合约漏洞检测}
\section{本章内容}
本章主要介绍了基于GraphCodeBERT模型的智能合约漏洞检测方法，并实验设计与结果。
\section{方法概述}
预训练模型可以学习并理解复杂的程序结构，并且具有跨语言通用性，大量工作表明预训练模型在漏洞检测任务中表现出色\cite{pretrained_is_good_1,pretrained_is_good_2,pretrained_is_good_3}。同时，Wu等人的研究表明预训练模型在识别智能合约的重入漏洞方面，明显优于当时最先进的方法\cite{wu2021peculiar}。遵循上述思路，本文拟采用GraphCodeBert模型为基础进行智能合约的漏洞检测研究。总的来说，本文的研究方法主要包含以下几个步骤：①计算专家特征。依据xxx节描述的专家特征，从静态代码分析的角度对智能合约进行表示。②构建语义图。依据xxx节描述的三种语义图，提取智能合约的语义信息。③基于GraphCodeBERT训练漏洞检测模型。以GraphCodeBERT模型为基础，融合前两步生成的专家特征和语义信息训练漏洞检测模型。
\autoref{fig:framework}图xxx显示了本方法的框架结构。接下来，本文将逐一介绍上述三个步骤。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.3\linewidth]{pictures/framework}
    \caption{\label{fig:framework}本方法的框架结构}
\end{figure}

\section{计算专家特征}
xxx节中获取的数据集并不包含合约的编译信息，为了获得xxx节中定义的专家特征，我们需要在本地编译Solidity源文件，然后构建抽象语法树再从中提取相关信息。py-solc-x\footnote{py-solc-x的网址}是solc\footnote{solc的网址}（Solidity编译器）的Python版本，py-solc-ast\footnote{py-solc-ast的网址}可以从solc编译后的信息中构建AST，并提供了API获取相关信息。

借助上述工具，我们提取出了xxx节中定义的基于静态代码度量的专家特征，其统计信息如\autoref{tab:new_dataset_statistics}所示。
\begin{table}[htbp]
    \caption{\label{tab:new_dataset_statistics}现有工作中使用的数据集的统计信息}
    \small
    \renewcommand{\arraystretch}{1.5}
    \begin{tabularx}{\linewidth}{cX<{\centering}X<{\centering}X<{\centering}}
        \hline
    数据集名称     & 智能合约数量 & 源代码与字节码 & 时间段            \\ \hline
    smartbugs & 47,518  & 源代码；字节码 & 2015$\sim$2019 \\
    crawID    & 321432 & 源代码     & 2015$\sim$2018 \\
    JiuZhou   & 54325  & 字节码     & 2015$\sim$2018 \\
    xxx       & 5435   & 源代码；字节码 & 2015$\sim$2019  \\  \hline
    \end{tabularx}
\end{table}
% 因此，我们在表 2 中列出了所研究合约的摘要。第一行显示了我们要分析的内容，包括从 Etherscan 上收集的 50994 个源代码文件（.sol 文件）中的智能合约、库、接口、事件、修改器和 LOC。如表所示，我们共分析了 225,918 份子合约（包括主合约及其附属合约）、32,165 个库、10,927 个接口、219,657 个事件、99,395 个修改器和 17,776,799 个 LOC。第二行显示的是每个 Solidity 源代码文件的平均统计信息。
% 平均而言，每个 Solidity 源文件有 4.43 个合约，这意味着开发人员更愿意将应用程序划分为更小的功能，进一步降低合约的复杂性。此外，每个 Solidity 源文件有 4.31 个事件，这意味着许多执行信息将被记录到以太坊中。修改器是 Solidity 中的函数包装器，可以改变操作顺序、检查权限、添加功能和重复使用代码。每个 Solidity 源文件包含 1.95 个修改器。库和接口相对较少，大约每两份和五份 Solidity 源代码就分别定义了一个库和一个接口。最后，我们发现平均每个 Solidity 源代码文件包含约 350 行代码，这表明一份功能完善的合同相对较小。
% 最后一行显示了这些研究的智能合约提交给 Etherscan 进行验证的日期。我们发现，大多数智能合约都是在 2018 年提交验证的，占研究合约的 82.7%（42180/50994）。
表 3 显示了所有研究智能合约的人工定义指标的统计数据。虽然标准偏差很大，但我们也可以得出一些一般性结论。
1) 根据 AvgCyclomatic 值，我们发现智能合约的复杂度较低。特别是，AvgCyclomatic 值为 1.34，这意味着大多数函数的控制结构都是顺序控制流，没有过多的条件判断。MaxNesting 的平均性能（即平均 1.45）也强调了这一结论。这些都不是智能合约中嵌套很深的控制结构。此外，CountContractCoupled 可衡量智能合约相互依赖的程度。根据 CountContractCoupled 的平均值（即 0.53）和第三四分位值（即 1），我们发现大多数合约在代码中不使用其他合约。
2) 平均而言，合约中的执行行数并不多（即约 198 行），这意味着智能合约的大小相对容易阅读。大多数合约都有很好的注释，每份合约中注释与代码的比例平均约为 24%。
3) 根据 MaxInheritanceTree 的值，平均而言，超过一半的智能合约至少有一个祖先（即平均数的 1.47 和中位数的 1）。此外，大多数智能合约至少有 1 个依赖关系（即 CountDependence 的 Q1）和 2 个基本合约（即 CountContractBase），这意味着大多数合约都是通过继承其他合约而发展起来的，合约之间的耦合关系是可以松散的。
4) 关于存储变量，我们发现它们的使用情况一般，平均值 NOSV 为 16.31，这意味着智能合约会给以太坊带来更多的存储空间。NOPay 的平均值和中位值分别为 1.1 和 1，这意味着大约一半的智能合约可能支持以太币的转移。NOT的平均值也加强了这一结论，这意味着约有一半的智能合约在其合约代码中进行了转账操作。
5) NOE 的第一四分位值和平均值分别为 2 和 4.31，这意味着大多数智能合约都定义了事件，因此重要的执行信息可以记录在 NOE 中。


以太坊至于 SDFB，平均值和中位值分别为 0.51 和 1，这意味着约有一半的智能合约将回退功能作为默认设置。
\section{构建语义图}
抽象语法树以结构化的形式记录了源代码中的关键信息，因此可以从Solidity源代码的AST中提取智能合约的语义信息，然后分别构建数据流图、控制流图和函数调用图。接下来本文将逐一介绍构建这三种语义图的方法。
\subsection{构建数据流图}
在xxx节中，我们已经获得了每个合约的抽象语法树，树的叶子节点记录的恰好是程序中的变量和字面量，将每个变量作为图中的一个节点，依据变量值在程序中的传递过程构建图中的有向边。变量的传递有两种方式，一种是直接引用，另一种是通过赋值语句。在图xxx中，从变量$MinDeposit^{17}$到变量$MinDeposit^{51}$ 的边属于第一种方式，称之为“comesFrom”类型，需要创建一条有向边$\left\langle MinDeposit^{17}, MinDeposit^{51}\right\rangle$。
以赋值语句$a=MaxValue-MinValue$为例，它属于第二种方式，称之为“computedFrom”类型，此时需要创建两条有向边$\left\langle MaxValue,a\right\rangle$和$\left\langle MinValue,a\right\rangle$。

更具体地构建数据流图的过程描述为：遍历AST的所有节点，如果当前节点是叶子节点且属于标识符类型（identifier），则将其添加到变量集合中；如果当前节点的类型是赋值语句类型\footnote{当前节点一定具有左右子节点，因为当前合约是可以编译通过的，那么一定符合该语法规则}（assignment\_expression），则创建一条由右子节点指向左子节点的有向边，重复上述过程直到遍历完成，最终可以得到当前合约的数据流图。
\subsection{构建函数调用图}
同样地，智能合约的函数调用图需要从AST中构建。在遍历AST的过程中，需要重点关注两种类型的节点，即函数定义类型（function\_definition)和函数调用（call\_expression）类型。

更具体地构建函数调用图的过程描述为：遍历AST的所有节点，同时使用栈数据结构记录当前路径上的函数标识符。如果当前遍历到的节点是函数定义类型，则将其添加到函数集合中；如果当前节点是函数调用类型，那说明栈顶标识符对应的函数调用了当前节点对应的函数，此时创建一条有向边。重复上述过程直到遍历完成，最终可以得到当前合约的函数调用图。
\subsection{构建控制流图}
控制流图依然需要从Solidity源代码的AST中构建，但与前两个语义图不同的是，控制流图的节点不再与AST中的节点相对应，前者应该是程序中的一个基本代码块，而不是一个标识符。在遍历AST的过程中，需要重点关注四种类型的节点：if语句、for语句、while语句和do-while语句，称为分支语句节点。遇到分支节点时需要将其拆分为3个代码块：条件代码块、条件为真时的基本代码块和条件为假时的基本代码块。

更具体地构建控制流图的过程描述为：遍历AST的所有节点，同时使用栈数据结构记录当前路径上经过的基本代码块。如果当前节点属于分支语句节点，则针对该分支语句解析出3个代码块，每个代码块均作为控制流图的节点，同时创建两条有向边，分别从条件代码块指向真代码块和假代码块；否则，创建一条有向边，从栈顶的基本代码块指向当前基本代码块。重复上述过程直到遍历完成，最终可以得到当前合约的控制流图。

\section{基于GraphCodeBERT的漏洞检测模型}
% 或者将语义特征与专家特征联合训练来构建模型
% 用源代码+语义图经过GraphCodeBERT模型生成高维向量，然后与专家特征结合，经过分类器
先前的预训练模型总是像对待文本一样去对待源代码，将其视为一连串token组成的序列，而忽视了源代码的复杂结构中蕴含的语义信息，于是GraphCodeBERT模型应运而生，它是第一个利用代码语义结构来学习代码表示的预训练模型\cite{guo2020graphcodebert}。最能表达程序语义信息的数据结构莫过于AST，但是AST作为树结构无法被直接输入到Transformer结构的模型中去，另一方面随着程序代码量的增多，AST的深度和宽度都会急剧增大。因此，GraphCodeBert模型采用了数据流图表征程序的语义信息。实验结果表明，引入了语义信息的GraphCodeBERT模型在多个下游任务上取得了最先进的效果。

但是不容忽视的是，单独的数据流图相对于整个AST会丢失相当的语义信息，这就导致程序的语义信息并没有被完全利用起来。于是GraphCodeBERT模型的原作者又在两年后提出了UniXcoder模型\cite{unixcoder}，他们构建了一个one-to-one的映射函数，用于将AST转为一个序列结构，然后与源代码、注释拼接后作为模型的输入进行预训练。

为了解决上述问题，本文拟在GraphCodeBERT模型的基础上，尝试在预训练的过程中增加更多的语义信息，即控制流图和函数调用图；另一方面，前文提出的静态代码指标也在一定维度上表达了智能合约的结构信息，因此本文将尝试融合静态代码指标与语义特征共同训练模型。

\subsection{模型架构}
本文遵循GraphCodeBERT模型的基本架构，即采用多层双向Transformer作为模型主干。与之不同的是，我们将在模型训练阶段融入更多的代码结构和语义信息。

具体来说，给定源代码$S=\{s_{1},s_{2},...,s_{n}\}$，可以利用xxx节描述的方法构建三种智能合约语义图，首先是数据流图$DFG(S)=(Var,DFG\_Edge)$，其中$Var=\{v_1,v_2,\ldots,v_k\}$是智能合约$S$的变量集合，$DFG\_Edge=\{\epsilon_1,\epsilon_2,~\ldots,~\epsilon_l\}$是有向边，表示每个变量的值来自哪里；然后是函数调用图$CG(S)=(Func,CG\_Edge)$，$Func=\{f_1,f_2,\ldots,f_s\}$是智能合约的函数集合，$CG\_Edge=\{\zeta_1,\zeta_2,~\ldots,~\zeta_l\}$是有向边，表示合约内不同函数的调用关系；最后还有控制流图$CFG(S)=(Block,CFG\_Edge)$，其中$Block=\{b_1,b_2,\ldots,b_t\}$表示合约内的基本代码块（即不含分支语句，可以顺序执行的代码），$CFG\_Edge=\{\iota_1,\iota_2,~\ldots,~\iota_l\}$表示是有向边，表示合约的执行路径。然后，遵循GraphCodeBERT模型的思路，将源代码和三种语义信息拼接成一个序列$I=\{[CLS],S,[SEP],Var,[SEP],Func,[SEP],Block\}$，其中$[CLS]$是每个输入序列前的特殊标记，$[SEP]$是不同数据段之间的分隔符。


下一步是将输入序列$I$转换为输入向量$H^0$。主要方法是，先对输入序列中的每个token生成嵌入向量（Embedding），然后将之与该token的位置嵌入向量相加，最终得到整个序列的输入向量。具体地，对于源代码$S$中的每个token，它的位置嵌入向量由该token在源代码中所处的位置序号生成；对于$Var$、$Func$、$Block$中的token，需要使用特殊的标记符生成位置嵌入向量，以此来表示他们是语义图中的节点，而非源代码中的字段。

输入向量$H^0$在经过$N$个Transformer层后被转换为上下文表示（Contextual Representation)，即$H^n=transformer_n(H^{n-1}),n\in[1,N]$，其中，每个Transformer层都包含一个完全相同的Transformer结构。向量$H^{n-1}$在$n-1$层经过Encoder编码和多头自注意力操作\cite{attention}后首先生成向量$G^n$（公式1），然后再经过一个前馈层生成向量$H^n$（公式2）。
$$G^n=LN(MultiAttn(H^{n-1})+H^{n-1})$$
$$H^n=LN(FFN(G^n)+G^n)$$
这里的LN是什么意思？
在公式1和公式2中，$MultiAttn$表示多头自注意力操作，$FFN$表示两层前馈网络，$LN$表示层归一化操作。
首先可以利用xxx节描述的方法计算静态代码指标，表示为$M=\{m_{1},m_{2},...,m_{m}\}$，静态代码指标与语义特征的融合将在下一小节详细阐述。
\autoref{fig:model_architecture}显示了本研究采用的模型架构。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.3\linewidth]{pictures/model_architecture}
    \caption{\label{fig:model_architecture}本研究采用的模型架构}
\end{figure}
描述一下模型架构
\subsection{特征融合}

\subsection{训练任务}
为了从源代码和数据流图中学习代码表示，他们引入了两种结构感知预训练任务，一种是数据流图的有向边预测任务，用于从数据流图中学习语义信息；另一种是源代码与数据流节点的对齐任务，用于映射源代码中的token与数据流图中的节点，从而保证源代码和数据流图在模型中的语义结构一致。
\section{实验设计}
描述实验步骤
在预训练阶段，模型学习了如何从图结构中提取有用的信息，并转化为向量表示（Embedding）

\subsection{实验环境}

\subsection{消融实验}

\subsection{评估指标}

\section{实验结果分析}
